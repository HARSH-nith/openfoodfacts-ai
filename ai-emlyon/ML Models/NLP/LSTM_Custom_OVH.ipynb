{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "9b2cb540d590a79081b820842456d76b03ebc469af0c9d4bf006b67d9ffd2015"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# LSTM $G_1$ & $G_2$ Classification with Pytorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Settings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!pip install -U spacy --upgrade\n",
    "!python -m spacy download en_core_web_trf\n",
    "!python -m spacy download fr_dep_news_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "from torchtext.legacy.data import LabelField\n",
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9.1\n"
     ]
    }
   ],
   "source": [
    "print(torchtext.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.8.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch ==1.8.1\n",
    "!pip install torchtext==0.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install torchtext\n",
    "!pip install sklearn"
   ]
  },
  {
   "source": [
    "## Preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = r'C:\\Users\\Antoine\\Coding Bootcamp\\Open Food Facts'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'{source_path}\\en_train_set.csv')\n",
    "data"
   ]
  },
  {
   "source": [
    "pkl_file_G1 = open(r'label_encoder_g1.pkl', 'rb')\n",
    "le = pickle.load(pkl_file_G1)\n",
    "pkl_file_G1.close()\n",
    "pkl_file_G2 = open(r'label_encoder_g2.pkl', 'rb')\n",
    "le_2 = pickle.load(pkl_file_G2)\n",
    "pkl_file_G2.close()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_labels(encoded_labels, label_vocab): return [label_vocab[str(code)] for code in encoded_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'labels_G1_code_reference.json') as json_file:\n",
    "    le_G1 = json.load(json_file)\n",
    "with open(r'labels_G2_code_reference.json') as json_file:\n",
    "    le_G2 = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_trf')\n",
    "def tokenizer(text): return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "source": [
    "## LSTM $G_1$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, lower=True, include_lengths=False, pad_token='<pad>', unk_token= '<unk>', batch_first=True, tokenize= tokenizer)\n",
    "LABELS = Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "fields = [('text', TEXT), ('label_G1', LABELS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = TabularDataset.splits(\n",
    "    path=source_path, \n",
    "    train='en_train_set.csv',\n",
    "    test='en_test_set.csv',\n",
    "    format='csv', \n",
    "    fields=fields, \n",
    "    skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(train, batch_size=256, sort_key=lambda x: len(x.text),\n",
    "device=device, sort=True, sort_within_batch=True, shuffle=True, repeat=False)\n",
    "\n",
    "valid_iter = BucketIterator(valid, batch_size=512, sort_key=lambda x: len(x.text),\n",
    "device=device, sort=True, sort_within_batch=True, shuffle=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, min_freq=5, vectors='glove.6B.300d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "source": [
    "## Architecture, Train & Eval Definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMG1(nn.Module):\n",
    "    def __init__(self, embedding_dim=300, hid_dim=50, n_layers=2, p=0.3, n_classes=9):\n",
    "        super(LSTMG1, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size = embedding_dim, \n",
    "            hidden_size = hid_dim, \n",
    "            num_layers = n_layers,\n",
    "            bidirectional = True,\n",
    "            batch_first = True)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.drop_emb = nn.Dropout(p/1.5)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=hid_dim)\n",
    "        self.hid_out = nn.Linear(hid_dim, n_classes)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embedding(inputs)\n",
    "        embeds_drop = self.drop(embeds)\n",
    "        outputs, (h_n, c_n) = self.lstm(embeds_drop)\n",
    "        x = self.drop(h_n[0])\n",
    "        x = self.bn1(x)\n",
    "        x = self.hid_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "        optimizer,\n",
    "        criterion = nn.CrossEntropyLoss(),\n",
    "        train_loader = train_iter,\n",
    "        valid_loader = valid_iter,\n",
    "        num_epochs = 5,\n",
    "        eval_every = len(train_iter) // 2,\n",
    "        best_valid_loss = float(\"Inf\")):\n",
    "          \n",
    "        running_loss = 0.0\n",
    "        valid_running_loss = 0.0\n",
    "        global_step = 0\n",
    "        train_loss_list = []\n",
    "        valid_loss_list = []\n",
    "        global_steps_list = []\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            for batch in train_loader:\n",
    "\n",
    "                data = batch.text.to(device)           \n",
    "                labels = batch.label.to(device)\n",
    "                output = model(data)\n",
    "    \n",
    "                loss = criterion(output, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                #update running vals\n",
    "                running_loss += loss.item()\n",
    "                global_step += 1\n",
    "                \n",
    "                #Eval step\n",
    "                \n",
    "                if global_step % eval_every == 0: model.eval()\n",
    "                \n",
    "                # validation loop\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    for batch in valid_loader:\n",
    "                        data = batch.text.to(device)\n",
    "                        labels = batch.label.to(device)\n",
    "                        output = model(data)\n",
    "                        \n",
    "                        loss = criterion(output, labels)\n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "                \n",
    "                # resetting running values\n",
    "                running_loss = 0.0\n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                # print progress\n",
    "                \n",
    "                print('Epoch [{}/{}], Step [{}/{}] - Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                average_train_loss, average_valid_loss))\n",
    "                print('-'*50)\n",
    "                \n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "            print('_'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_test, y_pred, sortby='precision', model='model'):\n",
    "    \"\"\"Return a classification report as pd.DataFrame\"\"\"\n",
    "    from sklearn import metrics\n",
    "    report = metrics.classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    df_classification_report = pd.DataFrame(report).transpose()\n",
    "    df_classification_report = df_classification_report.sort_values(by=[sortby], ascending=False)\n",
    "    df_classification_report.rename(columns={colname: model + '_' + colname for colname in df_classification_report.columns}, inplace=True)\n",
    "    return df_classification_report.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_iter, le):\n",
    "    y_true = []\n",
    "    y_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            data = batch.text.to(device)\n",
    "            labels = batch.label.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_preds.append(predicted.cpu().numpy())\n",
    "            y_true.append(labels.cpu().numpy())\n",
    "    y_true = np.concatenate(decode_labels(y_true, le))\n",
    "    y_preds = np.concatenate(decode_labels(y_preds, le))\n",
    "    report = get_classification_report(y_true, y_preds, model='BI_LSTM_G1')\n",
    "    return report"
   ]
  },
  {
   "source": [
    "## Training $G_1$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_G1 = LSTMG1(embedding_dim=300, n_layers=3, n_classes=9).to(device)\n",
    "optimizer = optim.Adam(net_G1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_G1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=net_G1, optimizer=optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(net_G1, valid_iter, le_G1) #le_G1"
   ]
  },
  {
   "source": [
    "## LSTM $G_2$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, lower=True, include_lengths=False, pad_token='<pad>', unk_token= '<unk>', batch_first=True, tokenize= tokenizer)\n",
    "LABELS = Field(sequential=False, use_vocab=False, batch_first=True)\n",
    "fields = [('text', TEXT), ('label_G2', LABELS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = TabularDataset.splits(\n",
    "    path=source_path, \n",
    "    train='en_train_set.csv',\n",
    "    test='en_test_set.csv',\n",
    "    format='csv', \n",
    "    fields=fields, \n",
    "    skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = BucketIterator(train, batch_size=256, sort_key=lambda x: len(x.text),\n",
    "device=device, sort=True, sort_within_batch=True, shuffle=True, repeat=False)\n",
    "\n",
    "valid_iter = BucketIterator(valid, batch_size=512, sort_key=lambda x: len(x.text),\n",
    "device=device, sort=True, sort_within_batch=True, shuffle=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, min_freq=5, vectors='glove.6B.300d')"
   ]
  },
  {
   "source": [
    "## Training $G_2$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_G2 = LSTMG1(embedding_dim=300, n_layers=3, n_classes=38).to(device)\n",
    "optimizer = optim.Adam(net_G2.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=net_G2, optimizer=optimizer, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(net_G2, valid_iter, le_G2) #le_G2"
   ]
  },
  {
   "source": [
    "## Export Models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net_G1.state_dict(), 'torch_lstm_G2')\n",
    "torch.save(net_G2.state_dict(), 'torch_lstm_G2')"
   ]
  }
 ]
}